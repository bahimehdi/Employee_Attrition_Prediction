# ğŸ¯ Employee Attrition Prediction - Machine Learning Project

> **Projet Data Science S7 | 2024-2025**  
> Analyse comparative de 12 modÃ¨les de Machine Learning pour la prÃ©diction du turnover des employÃ©s

## ğŸ“Š AperÃ§u du Projet

- **Dataset**: IBM HR Employee Attrition (1,470 employÃ©s, 35 colonnes)
- **Objectif**: PrÃ©dire si un employÃ© quittera l'entreprise (classification binaire)
- **Meilleur modÃ¨le**: Voting Classifier (**87.07%** accuracy)
- **RÃ©duction de features**: 34 â†’ 12 variables clÃ©s
- **Interface**: GUI interactive moderne (Tkinter)

## ğŸ† RÃ©sultats Principaux

| Rang | ModÃ¨le | Accuracy | Precision | Recall | F1-Score |
|:---:|---|:---:|:---:|:---:|:---:|
| **1** | **Voting Classifier** | **87.07%** | 76.47% | 27.66% | 40.62% |
| 2 | Logistic Regression | 86.73% | 75.00% | 25.53% | 38.10% |
| 3 | XGBoost | 86.39% | 66.67% | 29.79% | 41.18% |
| 4 | SVM (RBF) | 85.71% | 72.73% | 17.02% | 27.59% |
| 5 | Gradient Descent | 85.03% | 54.29% | 40.43% | 46.34% |
| 6 | Decision Tree | 84.35% | 51.35% | 40.43% | 45.24% |
| 7-10 | K-NN / Naive Bayes / RF | 84.01% | 50.00% | varies | varies |
| 11 | Tuned Random Forest | 84.35% | 53.85% | 14.89% | 23.33% |
| 12 | K-Means* | 84.01% | 0.00% | 0.00% | 0.00% |

*K-Means est non-supervisÃ©, utilisÃ© comme baseline de comparaison

## ğŸ“ Structure du Projet

```
final_submission/
â”œâ”€â”€ ğŸ“‚ GUI/
â”‚   â””â”€â”€ gui_attrition.py              # Application GUI interactive
â”œâ”€â”€ ğŸ“‚ Graphes/
â”‚   â”œâ”€â”€ model_comparison.png          # Comparaison des modÃ¨les
â”‚   â”œâ”€â”€ feature_importance_all.png    # Importance des features
â”‚   â”œâ”€â”€ knn_elbow_method.png          # MÃ©thode du coude K-NN
â”‚   â”œâ”€â”€ conclusion_*.png              # Graphes de conclusion
â”‚   â””â”€â”€ eda_plots/                    # Visualisations EDA
â”œâ”€â”€ ğŸ“‚ Rapport et prÃ©sentation/
â”‚   â”œâ”€â”€ rapport_attrition.pdf         # Rapport PDF compilÃ©
â”‚   â”œâ”€â”€ rapport_attrition.tex         # Source LaTeX du rapport
â”‚   â””â”€â”€ powerpointAttrition.pptx      # PrÃ©sentation PowerPoint
â”œâ”€â”€ employee_attrition.ipynb          # Notebook Jupyter complet
â”œâ”€â”€ model_comparison_results.csv      # RÃ©sultats des modÃ¨les
â”œâ”€â”€ WA_Fn-UseC_-HR-Employee-Attrition.csv  # Dataset IBM HR
â””â”€â”€ README.md                         # Ce fichier
```

## ğŸ” Insights ClÃ©s

### Top 5 PrÃ©dicteurs d'Attrition
1. **OverTime** - Indicateur le plus fort (risque Ã—3)
2. **YearsAtCompany** - AnciennetÃ© critique
3. **MonthlyIncome** - Salaires bas = mobilitÃ© accrue
4. **Age** - Jeunes employÃ©s plus Ã  risque
5. **DistanceFromHome** - Trajet domicile-travail

### Recommandations RH
- ğŸš¨ Les employÃ©s en heures supplÃ©mentaires ont un taux d'attrition **3Ã— supÃ©rieur**
- ğŸ’° Les employÃ©s partis gagnaient **30% de moins** en moyenne
- â³ **Les 2 premiÃ¨res annÃ©es** sont critiques pour la rÃ©tention
- ğŸ‘¥ Les moins de 35 ans sont plus Ã  risque

## ğŸš€ Installation & ExÃ©cution

### PrÃ©requis
```bash
Python 3.8+
pip install pandas numpy matplotlib seaborn scikit-learn xgboost
```

### ExÃ©cuter la GUI
```bash
cd GUI
python gui_attrition.py
```

### ExÃ©cuter le Notebook
Ouvrir `employee_attrition.ipynb` dans Jupyter ou Google Colab.

## ğŸ”¬ MÃ©thodologie

1. **Chargement & EDA** - Analyse exploratoire complÃ¨te
2. **Feature Engineering** - RÃ©duction 34â†’12 features, encodage, normalisation
3. **Train-Test Split** - 80/20 avec stratification
4. **12 ModÃ¨les** - EntraÃ®nement et Ã©valuation comparative
5. **Cross-Validation** - Validation croisÃ©e 5-fold
6. **Grid Search** - Optimisation hyperparamÃ¨tres Random Forest

## ğŸ‘¥ Auteurs

- **Mehdi BAHI**
- **Mustapha MELLAKI**

## ğŸ“„ Licence

MIT License - See [LICENSE](LICENSE) file  
Dataset: [IBM HR Analytics Attrition Dataset (Kaggle)](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset)