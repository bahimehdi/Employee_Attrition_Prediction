{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7cc708",
   "metadata": {},
   "source": [
    "# Employee Attrition ML (IBM HR) — Notebook\n",
    "Ce notebook reprend exactement le pipeline du script `employee_attrition_ml.py`, mais **sans classes ni méthodes** (code linéaire), découpé en cellules exécutables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports + paramètres généraux (graphiques, warnings, métriques)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "# XGBoost (optionnel)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"Warning: XGBoost not installed. Install with: pip install xgboost\")\n",
    "\n",
    "# Style des visualisations (comme dans le script)\n",
    "sns.set_theme(style=\"darkgrid\", palette=\"crest_r\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Fix Windows console encoding for UTF-8 output (si besoin)\n",
    "if sys.platform == 'win32':\n",
    "    try:\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bf0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation du dataset + infos de base (shape, distribution Attrition)\n",
    "DATA_PATH = \"WA_Fn-UseC_-HR-Employee-Attrition.csv\"  # adapte le chemin si besoin\n",
    "\n",
    "df_original = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING EMPLOYEE ATTRITION DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[OK] Dataset loaded successfully\")\n",
    "print(f\"  Shape: {df_original.shape[0]} rows  x  {df_original.shape[1]} columns\")\n",
    "print(f\"\\n  Attrition distribution:\")\n",
    "print(df_original['Attrition'].value_counts())\n",
    "print(f\"\\n  Attrition rate: {df_original['Attrition'].value_counts(normalize=True)['Yes']:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ea7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structuration / inspection générale + règles d'imputation (robustesse)\n",
    "df = df_original.copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RIGOROUS DATA STRUCTURING & EDA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. INSPECTION GÉNÉRALE (Data Overview)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Data Shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nNull Values per Column:\")\n",
    "null_counts = df.isnull().sum()\n",
    "print(null_counts[null_counts > 0] if null_counts.sum() > 0 else \"  No missing values found.\")\n",
    "\n",
    "print(\"\\n2. EXPLICIT CLEANING & IMPUTATION\")\n",
    "print(\"-\" * 40)\n",
    "print(\"  Even if clean, we enforce imputation rules for robustness:\")\n",
    "\n",
    "# Impute Categorical (Mode)\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "print(f\"  Imputing Categorical Columns (Mode): {list(cat_cols[:3])}...\")\n",
    "for col in cat_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Impute Numerical (Median)\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(f\"  Imputing Numerical Columns (Median): {list(num_cols[:3])}...\")\n",
    "for col in num_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "print(\"[OK] Imputation step complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement des outliers (méthode IQR) sur les variables clés (winsorisation)\n",
    "print(\"\\n3. OUTLIER TREATMENT (IQR Method)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "target_outlier_cols = ['MonthlyIncome', 'TotalWorkingYears', 'YearsAtCompany', 'Age', 'DistanceFromHome']\n",
    "\n",
    "for col in target_outlier_cols:\n",
    "    if col in df.columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers_low = (df[col] < lower_bound).sum()\n",
    "        outliers_high = (df[col] > upper_bound).sum()\n",
    "\n",
    "        if outliers_low > 0 or outliers_high > 0:\n",
    "            print(f\"  [OUTLIERS] {col}: Capping {outliers_low} low and {outliers_high} high values\")\n",
    "            df[col] = np.where(\n",
    "                df[col] < lower_bound, lower_bound,\n",
    "                np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  [CLEAN] {col}: No outliers detected inside 1.5*IQR range\")\n",
    "\n",
    "print(\"[OK] Outlier treatment complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b27442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphes EDA (1) : distributions (histogrammes) — sauvegardés dans eda_plots/\n",
    "import os\n",
    "os.makedirs('eda_plots', exist_ok=True)\n",
    "\n",
    "print(\"\\n4. GENERATING DETAILED VISUALIZATIONS\")\n",
    "print(\"-\" * 40)\n",
    "print(\"  A. Generating Histograms...\")\n",
    "\n",
    "for col in ['Age', 'MonthlyIncome', 'DistanceFromHome', 'YearsAtCompany']:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[col], kde=True, bins=30, color='skyblue')\n",
    "    plt.title(f\"Distribution: {col}\")\n",
    "    plt.savefig(f\"eda_plots/hist_{col}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphes EDA (2) : boxplots vs Attrition — sauvegardés dans eda_plots/\n",
    "print(\"  B. Generating Boxplots (vs Attrition)...\")\n",
    "\n",
    "for col in ['Age', 'MonthlyIncome', 'YearsAtCompany']:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=\"Attrition\", y=col, data=df, palette=\"Set2\")\n",
    "    plt.title(f\"{col} vs Attrition\")\n",
    "    plt.savefig(f\"eda_plots/box_{col}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphes EDA (3) : corrélation (features numériques) — sauvegardée dans eda_plots/\n",
    "print(\"  C. Generating Correlation Matrix...\")\n",
    "\n",
    "num_only = df.select_dtypes(include=['number'])\n",
    "if not num_only.empty:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_mx = num_only.corr()\n",
    "    sns.heatmap(corr_mx, annot=False, cmap='coolwarm', cbar=True)\n",
    "    plt.title(\"Full Feature Correlation Matrix\")\n",
    "    plt.savefig(\"eda_plots/correlation_matrix_full.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numerical columns found for correlation matrix.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphes EDA (4) : pairplot (variables clés) — sauvegardé dans eda_plots/\n",
    "print(\"  D. Generating Pairplot (Key Features)...\")\n",
    "\n",
    "key_vars = ['Age', 'MonthlyIncome', 'TotalWorkingYears', 'YearsAtCompany']\n",
    "df_pair = df[key_vars + ['Attrition']].copy()\n",
    "\n",
    "sns.pairplot(df_pair, hue=\"Attrition\", diag_kind=\"kde\", palette=\"bright\")\n",
    "plt.savefig(\"eda_plots/pairplot_key_features.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[OK] All rigorous EDA steps completed. Plots saved in 'eda_plots/' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be119bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction des features : 35 → 12 (constantes + corrélation + domain knowledge)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE REDUCTION: 35 → 12\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_original_clean = df.copy()\n",
    "df_reduction = df_original_clean.copy()\n",
    "\n",
    "feature_status = []  # dicts: Feature, Status, Reason, Correlation\n",
    "\n",
    "# 1) Colonnes constantes\n",
    "print(\"\\n1. Analyzing Constant/Single-Value Columns:\")\n",
    "constant_cols = []\n",
    "for col in df_reduction.columns:\n",
    "    if df_reduction[col].nunique() <= 1:\n",
    "        constant_cols.append(col)\n",
    "        feature_status.append({'Feature': col, 'Status': 'Dropped', 'Reason': 'Single Value (Constant)', 'Correlation': 0.0})\n",
    "\n",
    "potential_constants = ['EmployeeCount', 'Over18', 'StandardHours']\n",
    "for col in potential_constants:\n",
    "    if col in df_reduction.columns and col not in constant_cols:\n",
    "        constant_cols.append(col)\n",
    "        feature_status.append({'Feature': col, 'Status': 'Dropped', 'Reason': 'Single Value (Constant)', 'Correlation': 0.0})\n",
    "\n",
    "print(f\"   Found {len(constant_cols)} constant columns: {constant_cols}\")\n",
    "\n",
    "# 2) Corrélations (encodage temporaire complet)\n",
    "print(\"\\n2. Analyzing Correlations for ALL Features:\")\n",
    "\n",
    "df_analysis = df_original_clean.copy()\n",
    "\n",
    "# Target encoding pour corrélation\n",
    "if df_analysis['Attrition'].dtype == 'object':\n",
    "    df_analysis['Attrition'] = df_analysis['Attrition'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Encode toutes les colonnes object (sauf Attrition)\n",
    "object_cols = df_analysis.select_dtypes(include=['object']).columns\n",
    "for col in object_cols:\n",
    "    if col != 'Attrition':\n",
    "        le_tmp = LabelEncoder()\n",
    "        df_analysis[col] = le_tmp.fit_transform(df_analysis[col].astype(str))\n",
    "\n",
    "df_analysis = df_analysis.drop(columns=constant_cols, errors='ignore')\n",
    "\n",
    "all_correlations = df_analysis.corrwith(df_analysis['Attrition']).sort_values(ascending=False)\n",
    "all_correlations = all_correlations.drop('Attrition', errors='ignore')\n",
    "\n",
    "features_to_keep = [\n",
    "    'Age', 'Gender', 'MonthlyIncome', 'JobSatisfaction',\n",
    "    'EnvironmentSatisfaction', 'WorkLifeBalance', 'OverTime',\n",
    "    'YearsAtCompany', 'JobRole', 'Department', 'MaritalStatus',\n",
    "    'DistanceFromHome'\n",
    "]\n",
    "\n",
    "for feature, corr_val in all_correlations.items():\n",
    "    if feature in constant_cols:\n",
    "        continue\n",
    "    if feature in features_to_keep:\n",
    "        status = 'Kept'\n",
    "        reason = 'Domain Importance & Correlation'\n",
    "    else:\n",
    "        status = 'Dropped'\n",
    "        if abs(corr_val) < 0.05:\n",
    "            reason = 'Low Correlation (<0.05)'\n",
    "        else:\n",
    "            reason = 'Redundant/Multicollinearity'\n",
    "    feature_status.append({'Feature': feature, 'Status': status, 'Reason': reason, 'Correlation': corr_val})\n",
    "\n",
    "df_status = pd.DataFrame(feature_status)\n",
    "df_status['AbsCorr'] = df_status['Correlation'].abs()\n",
    "df_status = df_status.sort_values(['Status', 'AbsCorr'], ascending=[False, False])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE REDUCTION REPORT (35 Features)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Feature':<25} {'Status':<10} {'Correlation':<12} {'Reason'}\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in df_status.iterrows():\n",
    "    print(f\"{row['Feature']:<25} {row['Status']:<10} {row['Correlation']:<12.4f} {row['Reason']}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total Features: {len(feature_status)}\")\n",
    "print(f\"Kept: {len(df_status[df_status['Status']=='Kept'])}\")\n",
    "print(f\"Dropped: {len(df_status[df_status['Status']=='Dropped'])}\")\n",
    "\n",
    "# Visualisation (feature_importance_all.png)\n",
    "plt.figure(figsize=(14, 10))\n",
    "plot_data = df_status.sort_values('Correlation', ascending=True)\n",
    "\n",
    "colors = ['green' if s == 'Kept' else 'red' for s in plot_data['Status']]\n",
    "plt.barh(plot_data['Feature'], plot_data['Correlation'], color=colors)\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "plt.legend(handles=[Patch(facecolor='green', label='Kept Features'),\n",
    "                    Patch(facecolor='red', label='Dropped Features')],\n",
    "           loc='lower right')\n",
    "\n",
    "plt.title('Feature Correlation with Attrition (All 34 Features)\\nGreen = Kept, Red = Dropped', fontsize=14)\n",
    "plt.xlabel('Correlation Coefficient with Attrition')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=0, color='black', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_all.png', dpi=300)\n",
    "plt.show()\n",
    "print(\"\\n[OK] Comprehensive feature graph saved to: feature_importance_all.png\")\n",
    "\n",
    "# Application de la réduction\n",
    "df_processed = df_original_clean[features_to_keep + ['Attrition']].copy()\n",
    "print(\"[OK] Reduced dataframe shape:\", df_processed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84554c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des features catégorielles + encodage de la target Attrition\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENCODING CATEGORICAL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_encoded = df_processed.copy()\n",
    "\n",
    "categorical_cols = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'Attrition' in categorical_cols:\n",
    "    categorical_cols.remove('Attrition')\n",
    "\n",
    "print(f\"\\nCategorical columns to encode: {categorical_cols}\")\n",
    "\n",
    "# Target encoding\n",
    "le_target = LabelEncoder()\n",
    "df_encoded['Attrition'] = le_target.fit_transform(df_encoded['Attrition'])\n",
    "print(f\"\\n[OK] Target encoding: Yes=1, No=0\")\n",
    "\n",
    "# Binary encoding Gender, OverTime\n",
    "if 'Gender' in df_encoded.columns:\n",
    "    df_encoded['Gender'] = df_encoded['Gender'].map({'Male': 0, 'Female': 1})\n",
    "    print(\"[OK] Gender encoding: Male=0, Female=1\")\n",
    "\n",
    "if 'OverTime' in df_encoded.columns:\n",
    "    df_encoded['OverTime'] = df_encoded['OverTime'].map({'No': 0, 'Yes': 1})\n",
    "    print(\"[OK] OverTime encoding: No=0, Yes=1\")\n",
    "\n",
    "# Label encoding autres colonnes catégorielles\n",
    "remaining_categorical = [c for c in categorical_cols if c not in ['Gender', 'OverTime']]\n",
    "encoders = {}\n",
    "\n",
    "for col in remaining_categorical:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "    encoders[col] = le\n",
    "    print(f\"[OK] {col} encoded: {len(le.classes_)} categories\")\n",
    "\n",
    "print(\"[OK] Encoding complete\")\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc9aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80% train / 20% test + scaling (comme dans le script)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X = df_encoded.drop('Attrition', axis=1)\n",
    "y = df_encoded['Attrition']\n",
    "\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n[OK] Split ratio: {int((1-test_size)*100)}-{int(test_size*100)}\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\n  Class distribution (training):\")\n",
    "print(f\"    No attrition (0): {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train):.2%})\")\n",
    "print(f\"    Attrition (1):    {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train):.2%})\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n[OK] Features scaled for distance-based models (K-NN, SVM)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c9a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de K (Elbow Method) — sauvegarde knn_elbow_method.png\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINDING OPTIMAL K (ELBOW METHOD)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "max_k = 40\n",
    "error_rates = []\n",
    "\n",
    "for i in range(1, max_k + 1):\n",
    "    knn_tmp = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_tmp.fit(X_train_scaled, y_train)\n",
    "    pred_i = knn_tmp.predict(X_test_scaled)\n",
    "    error_rates.append(np.mean(pred_i != y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, max_k + 1), error_rates, color='blue', linestyle='dashed',\n",
    "         marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value (Elbow Method)')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.savefig('knn_elbow_method.png')\n",
    "plt.show()\n",
    "\n",
    "optimal_k = error_rates.index(min(error_rates)) + 1\n",
    "print(f\"Minimum error rate: {min(error_rates):.4f} at K={optimal_k}\")\n",
    "print(\"Chosen K=5 is a common default that often balances bias/variance well.\")\n",
    "print(\"[OK] Elbow Method plot saved to 'knn_elbow_method.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des conteneurs pour stocker les modèles + résultats (métriques)\n",
    "models = {}\n",
    "results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763eac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1: K-NN (k=5) — entraînement + évaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 1: K-NEAREST NEIGHBORS (K=5)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_NAME = \"K-NN (k=5)\"\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "models[MODEL_NAME] = model\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "results[MODEL_NAME] = metrics\n",
    "\n",
    "print(f\"\\n  Accuracy:   {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision:  {metrics['Precision']:.4f} ({metrics['Precision']*100:.2f}%)\")\n",
    "print(f\"  Recall:     {metrics['Recall']:.4f} ({metrics['Recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score:   {metrics['F1-Score']:.4f} ({metrics['F1-Score']*100:.2f}%)\")\n",
    "cm = metrics['Confusion Matrix']\n",
    "print(\"\\n  Confusion Matrix:\")\n",
    "print(f\"    [[TN={cm[0,0]:<4} FP={cm[0,1]:<4}]\")\n",
    "print(f\"     [FN={cm[1,0]:<4} TP={cm[1,1]:<4}]]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2: K-NN Weighted (distance) — entraînement + évaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 2: K-NEAREST NEIGHBORS WEIGHTED (DISTANCE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_NAME = \"K-NN Weighted\"\n",
    "model = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "models[MODEL_NAME] = model\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "results[MODEL_NAME] = metrics\n",
    "\n",
    "print(f\"\\n  Accuracy:   {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision:  {metrics['Precision']:.4f} ({metrics['Precision']*100:.2f}%)\")\n",
    "print(f\"  Recall:     {metrics['Recall']:.4f} ({metrics['Recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score:   {metrics['F1-Score']:.4f} ({metrics['F1-Score']*100:.2f}%)\")\n",
    "cm = metrics['Confusion Matrix']\n",
    "print(\"\\n  Confusion Matrix:\")\n",
    "print(f\"    [[TN={cm[0,0]:<4} FP={cm[0,1]:<4}]\")\n",
    "print(f\"     [FN={cm[1,0]:<4} TP={cm[1,1]:<4}]]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 3: SVM (RBF) — entraînement + évaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 3: SUPPORT VECTOR MACHINE (RBF KERNEL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_NAME = \"SVM (RBF)\"\n",
    "model = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "models[MODEL_NAME] = model\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "results[MODEL_NAME] = metrics\n",
    "\n",
    "print(f\"\\n  Accuracy:   {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision:  {metrics['Precision']:.4f} ({metrics['Precision']*100:.2f}%)\")\n",
    "print(f\"  Recall:     {metrics['Recall']:.4f} ({metrics['Recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score:   {metrics['F1-Score']:.4f} ({metrics['F1-Score']*100:.2f}%)\")\n",
    "cm = metrics['Confusion Matrix']\n",
    "print(\"\\n  Confusion Matrix:\")\n",
    "print(f\"    [[TN={cm[0,0]:<4} FP={cm[0,1]:<4}]\")\n",
    "print(f\"     [FN={cm[1,0]:<4} TP={cm[1,1]:<4}]]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 4: Naive Bayes (Gaussian) — entraînement + évaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 4: NAIVE BAYES (GAUSSIAN)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_NAME = \"Naive Bayes\"\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "models[MODEL_NAME] = model\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "results[MODEL_NAME] = metrics\n",
    "\n",
    "print(f\"\\n  Accuracy:   {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision:  {metrics['Precision']:.4f} ({metrics['Precision']*100:.2f}%)\")\n",
    "print(f\"  Recall:     {metrics['Recall']:.4f} ({metrics['Recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score:   {metrics['F1-Score']:.4f} ({metrics['F1-Score']*100:.2f}%)\")\n",
    "cm = metrics['Confusion Matrix']\n",
    "print(\"\\n  Confusion Matrix:\")\n",
    "print(f\"    [[TN={cm[0,0]:<4} FP={cm[0,1]:<4}]\")\n",
    "print(f\"     [FN={cm[1,0]:<4} TP={cm[1,1]:<4}]]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faafe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 5: Decision Tree — entraînement + évaluation + top features\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 5: DECISION TREE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_NAME = \"Decision Tree\"\n",
    "model = DecisionTreeClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "models[MODEL_NAME] = model\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "results[MODEL_NAME] = metrics\n",
    "\n",
    "print(f\"\\n  Accuracy:   {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision:  {metrics['Precision']:.4f} ({metrics['Precision']*100:.2f}%)\")\n",
    "print(f\"  Recall:     {metrics['Recall']:.4f} ({metrics['Recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score:   {metrics['F1-Score']:.4f} ({metrics['F1-Score']*100:.2f}%)\")\n",
    "cm = metrics['Confusion Matrix']\n",
    "print(\"\\n  Confusion Matrix:\")\n",
    "print(f\"    [[TN={cm[0,0]:<4} FP={cm[0,1]:<4}]\")\n",
    "print(f\"     [FN={cm[1,0]:<4} TP={cm[1,1]:<4}]]\")\n",
    "\n",
    "\n",
    "# Feature importance (top 5)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\n  Top 5 Important Features:\")\n",
    "for _, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"    {row['Feature']:<25} {row['Importance']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 6: Random Forest — entraînement + évaluation + top features\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 6: RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_NAME = \"Random Forest\"\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "models[MODEL_NAME] = model\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "results[MODEL_NAME] = metrics\n",
    "\n",
    "print(f\"\\n  Accuracy:   {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision:  {metrics['Precision']:.4f} ({metrics['Precision']*100:.2f}%)\")\n",
    "print(f\"  Recall:     {metrics['Recall']:.4f} ({metrics['Recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score:   {metrics['F1-Score']:.4f} ({metrics['F1-Score']*100:.2f}%)\")\n",
    "cm = metrics['Confusion Matrix']\n",
    "print(\"\\n  Confusion Matrix:\")\n",
    "print(f\"    [[TN={cm[0,0]:<4} FP={cm[0,1]:<4}]\")\n",
    "print(f\"     [FN={cm[1,0]:<4} TP={cm[1,1]:<4}]]\")\n",
    "\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\n  Top 5 Important Features:\")\n",
    "for _, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"    {row['Feature']:<25} {row['Importance']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1860ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 7: Gradient Descent (SGDClassifier) — entraînement + évaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 7: GRADIENT DESCENT (SGD CLASSIFIER)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_NAME = \"Gradient Descent\"\n",
    "model = SGDClassifier(loss='log_loss', max_iter=1000, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "models[MODEL_NAME] = model\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "results[MODEL_NAME] = metrics\n",
    "\n",
    "print(f\"\\n  Accuracy:   {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision:  {metrics['Precision']:.4f} ({metrics['Precision']*100:.2f}%)\")\n",
    "print(f\"  Recall:     {metrics['Recall']:.4f} ({metrics['Recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score:   {metrics['F1-Score']:.4f} ({metrics['F1-Score']*100:.2f}%)\")\n",
    "cm = metrics['Confusion Matrix']\n",
    "print(\"\\n  Confusion Matrix:\")\n",
    "print(f\"    [[TN={cm[0,0]:<4} FP={cm[0,1]:<4}]\")\n",
    "print(f\"     [FN={cm[1,0]:<4} TP={cm[1,1]:<4}]]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 8: K-Means (unsupervised) — implémentation manuelle inline (sans def/class)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 8: K-MEANS CLUSTERING (UNSUPERVISED)\")\n",
    "print(\"=\"*80)\n",
    "print(\"  Using manual K-Means implementation (Windows compatibility)\")\n",
    "\n",
    "MODEL_NAME = \"K-Means\"\n",
    "n_clusters = 2\n",
    "max_iters = 100\n",
    "np.random.seed(42)\n",
    "\n",
    "Xtr = X_train_scaled\n",
    "Xte = X_test_scaled\n",
    "\n",
    "n_samples = Xtr.shape[0]\n",
    "indices = np.random.choice(n_samples, n_clusters, replace=False)\n",
    "centroids = Xtr[indices].copy()\n",
    "\n",
    "for _ in range(max_iters):\n",
    "    # distances shape: (k, n_samples)\n",
    "    distances = np.sqrt(((Xtr - centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "    labels = np.argmin(distances, axis=0)\n",
    "\n",
    "    new_centroids = np.array([Xtr[labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "    if np.allclose(centroids, new_centroids):\n",
    "        break\n",
    "    centroids = new_centroids\n",
    "\n",
    "# predict on test\n",
    "distances_test = np.sqrt(((Xte - centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "test_labels = np.argmin(distances_test, axis=0)\n",
    "\n",
    "# align clusters with actual labels (majority vote)\n",
    "cluster_labels = {}\n",
    "ytr_arr = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "for cluster_id in range(n_clusters):\n",
    "    mask = labels == cluster_id\n",
    "    if mask.sum() > 0:\n",
    "        majority_label = np.bincount(ytr_arr[mask]).argmax()\n",
    "        cluster_labels[cluster_id] = majority_label\n",
    "\n",
    "y_pred = np.array([cluster_labels.get(c, 0) for c in test_labels])\n",
    "\n",
    "# store centroids + mapping as a tuple (pas de classe)\n",
    "models[MODEL_NAME] = (centroids, cluster_labels)\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "results[MODEL_NAME] = metrics\n",
    "\n",
    "print(f\"\\n  Accuracy:   {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision:  {metrics['Precision']:.4f} ({metrics['Precision']*100:.2f}%)\")\n",
    "print(f\"  Recall:     {metrics['Recall']:.4f} ({metrics['Recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score:   {metrics['F1-Score']:.4f} ({metrics['F1-Score']*100:.2f}%)\")\n",
    "cm = metrics['Confusion Matrix']\n",
    "print(\"\\n  Confusion Matrix:\")\n",
    "print(f\"    [[TN={cm[0,0]:<4} FP={cm[0,1]:<4}]\")\n",
    "print(f\"     [FN={cm[1,0]:<4} TP={cm[1,1]:<4}]]\")\n",
    "\n",
    "\n",
    "print(\"\\n  Note: K-Means is unsupervised; performance may be lower than supervised methods.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfa1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 9: Logistic Regression — entraînement + évaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 9: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_NAME = \"Logistic Regression\"\n",
    "model = LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "models[MODEL_NAME] = model\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "results[MODEL_NAME] = metrics\n",
    "\n",
    "print(f\"\\n  Accuracy:   {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision:  {metrics['Precision']:.4f} ({metrics['Precision']*100:.2f}%)\")\n",
    "print(f\"  Recall:     {metrics['Recall']:.4f} ({metrics['Recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score:   {metrics['F1-Score']:.4f} ({metrics['F1-Score']*100:.2f}%)\")\n",
    "cm = metrics['Confusion Matrix']\n",
    "print(\"\\n  Confusion Matrix:\")\n",
    "print(f\"    [[TN={cm[0,0]:<4} FP={cm[0,1]:<4}]\")\n",
    "print(f\"     [FN={cm[1,0]:<4} TP={cm[1,1]:<4}]]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40869903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 10: XGBoost — entraînement + évaluation (si disponible)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 10: XGBOOST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_NAME = \"XGBoost\"\n",
    "\n",
    "if not XGBOOST_AVAILABLE:\n",
    "    print(\"  XGBoost not available - skipping\")\n",
    "else:\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    models[MODEL_NAME] = model\n",
    "    metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "results[MODEL_NAME] = metrics\n",
    "\n",
    "print(f\"\\n  Accuracy:   {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision:  {metrics['Precision']:.4f} ({metrics['Precision']*100:.2f}%)\")\n",
    "print(f\"  Recall:     {metrics['Recall']:.4f} ({metrics['Recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score:   {metrics['F1-Score']:.4f} ({metrics['F1-Score']*100:.2f}%)\")\n",
    "cm = metrics['Confusion Matrix']\n",
    "print(\"\\n  Confusion Matrix:\")\n",
    "print(f\"    [[TN={cm[0,0]:<4} FP={cm[0,1]:<4}]\")\n",
    "print(f\"     [FN={cm[1,0]:<4} TP={cm[1,1]:<4}]]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe001fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table de comparaison des modèles (Accuracy/Precision/Recall/F1) + export CSV\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = []\n",
    "for model_name, metrics in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': f\"{metrics['Accuracy']*100:.2f}%\",\n",
    "        'Precision': f\"{metrics['Precision']*100:.2f}%\",\n",
    "        'Recall': f\"{metrics['Recall']*100:.2f}%\",\n",
    "        'F1-Score': f\"{metrics['F1-Score']*100:.2f}%\"\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "df_comparison['Accuracy_num'] = df_comparison['Accuracy'].str.rstrip('%').astype(float)\n",
    "df_comparison = df_comparison.sort_values('Accuracy_num', ascending=False).drop('Accuracy_num', axis=1)\n",
    "df_comparison.insert(0, 'Rank', range(1, len(df_comparison) + 1))\n",
    "\n",
    "print(\"\\n\" + df_comparison.to_string(index=False))\n",
    "\n",
    "output_file = 'model_comparison_results.csv'\n",
    "df_comparison.to_csv(output_file, index=False)\n",
    "print(f\"\\n[OK] Results saved to: {output_file}\")\n",
    "\n",
    "df_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphe : comparaison des Accuracy (bar chart horizontal) — sauvegarde model_comparison.png\n",
    "comparison_plot = []\n",
    "for model_name, metrics in results.items():\n",
    "    comparison_plot.append({'Model': model_name, 'Accuracy': metrics['Accuracy'] * 100})\n",
    "\n",
    "df_plot = pd.DataFrame(comparison_plot).sort_values('Accuracy', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.barh(df_plot['Model'], df_plot['Accuracy'], color='steelblue')\n",
    "\n",
    "# Color the best model\n",
    "bars[-1].set_color('darkgreen')\n",
    "\n",
    "plt.xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Model', fontsize=12, fontweight='bold')\n",
    "plt.title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlim(0, 100)\n",
    "\n",
    "for i, (_, row) in enumerate(df_plot.iterrows()):\n",
    "    plt.text(row['Accuracy'] + 1, i, f\"{row['Accuracy']:.2f}%\", va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"[OK] Comparison chart saved to: model_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphes : matrices de confusion pour tous les modèles — sauvegarde dans confusion_matrices/\n",
    "import os\n",
    "os.makedirs('confusion_matrices', exist_ok=True)\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    cm = metrics['Confusion Matrix']\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues',\n",
    "        xticklabels=['No Attrition', 'Attrition'],\n",
    "        yticklabels=['No Attrition', 'Attrition'],\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "    plt.title(f'Confusion Matrix: {model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "    filename = f\"confusion_matrices/{model_name.replace(' ', '_').replace('(', '').replace(')', '').replace('=', '')}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"[OK] Confusion matrices saved to: confusion_matrices/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b29b47",
   "metadata": {},
   "source": [
    "## Fichiers générés\n",
    "- `eda_plots/` (histogrammes, boxplots, corrélation, pairplot)  \n",
    "- `feature_importance_all.png`  \n",
    "- `knn_elbow_method.png`  \n",
    "- `model_comparison_results.csv`  \n",
    "- `model_comparison.png`  \n",
    "- `confusion_matrices/` (PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sauvegarde du modèle Logistic Regression (joblib)\n",
    "# Cette cellule permet d'enregistrer le modèle entraîné afin de le réutiliser ultérieurement\n",
    "\n",
    "import joblib\n",
    "\n",
    "MODEL_NAME = \"Logistic Regression\"\n",
    "\n",
    "if MODEL_NAME not in models:\n",
    "    raise ValueError(\"Le modèle Logistic Regression n'existe pas dans le dictionnaire models.\")\n",
    "\n",
    "logistic_model = models[MODEL_NAME]\n",
    "\n",
    "model_filename = \"logistic_regression_employee_attrition.pkl\"\n",
    "joblib.dump(logistic_model, model_filename)\n",
    "\n",
    "print(f\"[OK] Modèle Logistic Regression sauvegardé avec succès : {model_filename}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
